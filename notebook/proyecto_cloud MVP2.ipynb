{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **2do parcial - MVP TECNICO**\n",
        "\n",
        "# Materia: Mineria de datos II\n",
        "\n",
        "###Alumno: Emilio Gomez Lencina\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Pipelines\n",
        "\n",
        "**(1) Batch**\n",
        "\n",
        "Landing ----batch---->Bronze (csv)--->Silver---> Gold ---> Serving (cassandra)\n",
        "\n",
        "**(2) Stream**\n",
        "\n",
        "Landing ----speed----> Bronze (jsonl)--->Silver --> gold (stream) --->Serving (Cassandra)\n",
        "\n",
        "**(3) Queries de cassandra**\n",
        "---\n",
        "\n",
        "Resumen de la implementación\n",
        "\n",
        "*Cada celda (con excepcion de los demos) corresponde a un archivo .py que eventualmente va a estar en el repo final al moment ode la entrega final.\n",
        "\n",
        "Por eso, se usaron parches como:\n",
        "\n",
        "\n",
        "```\n",
        "try:\n",
        "    from cassandra_utils import get_cassandra_session, KEYSPACE\n",
        "except ModuleNotFoundError:\n",
        "    pass\n",
        "```\n",
        "En el archivo .py solo va a estar la llamada a cassandra_utils, pero como en el colab no es necesario, epuse ese parchecito temporal solo al efecto de la dentrega de este MVP.\n",
        "\n",
        "\n",
        "Este notebook contiene un end-to-end que incluye:\n",
        "\n",
        "    *implementacion de left anti joins para manejo de cuarentena (evitando duplicidad de errores) y estrategias de escritura segura.\n",
        "\n",
        "    *reglas de de negocio activas (costos negativos, integridad referencial) y desvío automático a zona de Quarantine.\n",
        "\n",
        "    *ptimizaciones Spark: Configuracion de shuffle.partitions para entorno local/Colab y uso de broadcast joins.\n",
        "\n",
        "    -*streaming con watermarking, dedupe y checkpointing para tolerancia a fallos.\n",
        "\n",
        "    *serving Layer: conexin a AstraDB (Cassandra) con modelado Query-First.\n",
        "\n",
        "\n",
        "Laburo completo aca ----->[Repositorio GitHub (Código + Readme + Diagramas) ](https://github.com/Sinnick4r/Cloud_Provider_Analytics_MVP)\n",
        "\n"
      ],
      "metadata": {
        "id": "X0ApXM5Cdb7s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "cUEIwMsZ5wJ8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46bbd8d3-ad64-4bf1-f990-dd66bda8ea08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] Dataset descomprimido en /content/datalake/landing\n"
          ]
        }
      ],
      "source": [
        "# config.py\n",
        "\n",
        "from __future__ import annotations\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "from typing import Final\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "\n",
        "\n",
        "# integracion con google drive que estuve usando, se puede poner true o false\n",
        "\n",
        "USE_GOOGLE_DRIVE: Final[bool] = False\n",
        "GOOGLE_DRIVE_PROJECT_SUBDIR: Final[str] = \"Mineria de datos II/Proyecto Cloud Provider Analysis\"\n",
        "\n",
        "\n",
        "\n",
        "def get_project_root() -> Path:\n",
        "    \"\"\"\n",
        "    Devuelve la raíz del proyecto.\n",
        "\n",
        "    - so USE_GOOGLE_DRIVE es true, monta gdrive en Colab y usa la carpeta indicada.\n",
        "    - si es Ffalse, usa el directorio actual (repo descomprimido).\n",
        "    \"\"\"\n",
        "    if USE_GOOGLE_DRIVE:\n",
        "        try:\n",
        "            from google.colab import drive as gdrive\n",
        "        except ImportError as exc:\n",
        "            raise RuntimeError(\n",
        "                \"USE_GOOGLE_DRIVE=True pero no esamos en colab.\"\n",
        "            ) from exc\n",
        "\n",
        "        gdrive.mount(\"/content/drive\")\n",
        "        return (Path(\"/content/drive/MyDrive\") / GOOGLE_DRIVE_PROJECT_SUBDIR).resolve()\n",
        "\n",
        "    return Path(\".\").resolve()\n",
        "\n",
        "#aca van loss directorios de todo el proyecto\n",
        "\n",
        "PROJECT_ROOT: Final[Path] = get_project_root()\n",
        "DATA_DIR: Final[Path] = PROJECT_ROOT / \"data\"\n",
        "DATALAKE_ROOT: Final[Path] = PROJECT_ROOT / \"datalake\"\n",
        "\n",
        "LANDING_PATH: Final[Path] = DATALAKE_ROOT / \"landing\"\n",
        "BRONZE_PATH: Final[Path] = DATALAKE_ROOT / \"bronze\"\n",
        "SILVER_PATH: Final[Path] = DATALAKE_ROOT / \"silver\"\n",
        "GOLD_PATH: Final[Path] = DATALAKE_ROOT / \"gold\"\n",
        "QUARANTINE_PATH: Final[Path] = DATALAKE_ROOT / \"quarantine\"\n",
        "\n",
        "RAW_ZIP_NAME: Final[str] = \"cloud_provider_challenge_dataset_v1.zip\"\n",
        "\n",
        "\n",
        "# sesion de park\n",
        "\n",
        "def create_spark(app_name: str = \"CloudProviderAnalytics_Pipeline\") -> SparkSession:\n",
        "    spark = (\n",
        "        SparkSession.builder\n",
        "        .appName(app_name)\n",
        "        .master(\"local[*]\")\n",
        "        .config(\"spark.sql.shuffle.partitions\", \"4\")\n",
        "        .getOrCreate()\n",
        "    )\n",
        "    spark.sparkContext.setLogLevel(\"WARN\")\n",
        "    return spark\n",
        "\n",
        "\n",
        "# utils de archivos/directorios\n",
        "\n",
        "def ensure_dirs() -> None:\n",
        "# Crea la estructura del datalake si no existe.\n",
        "    for path in (LANDING_PATH, BRONZE_PATH, SILVER_PATH, GOLD_PATH, QUARANTINE_PATH):\n",
        "        path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "\n",
        "def unpack_raw_dataset() -> None:\n",
        "\n",
        "    #descomprime el zip del datalake\n",
        "    zip_path = DATA_DIR / RAW_ZIP_NAME\n",
        "\n",
        "\n",
        "    if not zip_path.exists():\n",
        "        print(f\"[WARN] No se encontró el ZIP de datos en {zip_path}. Saltando unpack.\")\n",
        "        return\n",
        "    with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
        "        zf.extractall(PROJECT_ROOT)\n",
        "\n",
        "print(f\"[OK] Dataset descomprimido en {PROJECT_ROOT / 'datalake' / 'landing'}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# descompresiondel archivo y chequeo\n",
        "\n",
        "print(f\"[INFO] PROJECT_ROOT: {PROJECT_ROOT}\")\n",
        "ensure_dirs()\n",
        "unpack_raw_dataset()\n",
        "spark = create_spark()\n",
        "print(f\"[INFO] Spark version: {spark.version}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "slT805lr54FU",
        "outputId": "6681099d-1509-4ef2-ff02-7a3a8bf52938"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] PROJECT_ROOT: /content\n",
            "[INFO] Spark version: 3.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# schemas.py\n",
        "\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "from typing import Final\n",
        "from pyspark.sql import types as T\n",
        "\n",
        "\n",
        "# Esquemascsv y json totalmente manuales despeus de chequearlos\n",
        "\n",
        "customers_orgs_schema: Final[T.StructType] = T.StructType([\n",
        "    T.StructField(\"org_id\", T.StringType(), nullable=False),\n",
        "    T.StructField(\"org_name\", T.StringType(), nullable=True),\n",
        "    T.StructField(\"industry\", T.StringType(), nullable=True),\n",
        "    T.StructField(\"hq_region\", T.StringType(), nullable=True),\n",
        "    T.StructField(\"plan_tier\", T.StringType(), nullable=True),\n",
        "    T.StructField(\"is_enterprise\", T.BooleanType(), nullable=True),\n",
        "    T.StructField(\"signup_date\", T.DateType(), nullable=True),\n",
        "    T.StructField(\"sales_rep\", T.StringType(), nullable=True),\n",
        "    T.StructField(\"lifecycle_stage\", T.StringType(), nullable=True),\n",
        "    T.StructField(\"marketing_source\", T.StringType(), nullable=True),\n",
        "    T.StructField(\"nps_score\", T.DoubleType(), nullable=True),\n",
        "])\n",
        "\n",
        "users_schema: Final[T.StructType] = T.StructType([\n",
        "    T.StructField(\"user_id\", T.StringType(), nullable=False),\n",
        "    T.StructField(\"org_id\", T.StringType(), nullable=False),\n",
        "    T.StructField(\"email\", T.StringType(), nullable=True),\n",
        "    T.StructField(\"role\", T.StringType(), nullable=True),\n",
        "    T.StructField(\"active\", T.BooleanType(), nullable=True),\n",
        "    T.StructField(\"created_at\", T.DateType(), nullable=True),\n",
        "    T.StructField(\"last_login\", T.DateType(), nullable=True),\n",
        "])\n",
        "\n",
        "resources_schema: Final[T.StructType] = T.StructType([\n",
        "    T.StructField(\"resource_id\", T.StringType(), nullable=False),\n",
        "    T.StructField(\"org_id\", T.StringType(), nullable=False),\n",
        "    T.StructField(\"service\", T.StringType(), nullable=True),\n",
        "    T.StructField(\"region\", T.StringType(), nullable=True),\n",
        "    T.StructField(\"created_at\", T.DateType(), nullable=True),\n",
        "    T.StructField(\"state\", T.StringType(), nullable=True),\n",
        "    T.StructField(\"tags_json\", T.StringType(), nullable=True),\n",
        "])\n",
        "\n",
        "support_tickets_schema: Final[T.StructType] = T.StructType([\n",
        "    T.StructField(\"ticket_id\", T.StringType(), nullable=False),\n",
        "    T.StructField(\"org_id\", T.StringType(), nullable=False),\n",
        "    T.StructField(\"category\", T.StringType(), nullable=True),\n",
        "    T.StructField(\"severity\", T.StringType(), nullable=True),\n",
        "    T.StructField(\"created_at\", T.DateType(), nullable=True),\n",
        "    T.StructField(\"resolved_at\", T.DateType(), nullable=True),\n",
        "    T.StructField(\"csat\", T.DoubleType(), nullable=True),\n",
        "    T.StructField(\"sla_breached\", T.BooleanType(), nullable=True),\n",
        "])\n",
        "\n",
        "marketing_touches_schema: Final[T.StructType] = T.StructType([\n",
        "    T.StructField(\"touch_id\", T.StringType(), nullable=False),\n",
        "    T.StructField(\"org_id\", T.StringType(), nullable=False),\n",
        "    T.StructField(\"campaign\", T.StringType(), nullable=True),\n",
        "    T.StructField(\"channel\", T.StringType(), nullable=True),\n",
        "    T.StructField(\"timestamp\", T.DateType(), nullable=True),\n",
        "    T.StructField(\"clicked\", T.BooleanType(), nullable=True),\n",
        "    T.StructField(\"converted\", T.BooleanType(), nullable=True),\n",
        "])\n",
        "\n",
        "nps_surveys_schema: Final[T.StructType] = T.StructType([\n",
        "    T.StructField(\"org_id\", T.StringType(), nullable=False),\n",
        "    T.StructField(\"survey_date\", T.DateType(), nullable=True),\n",
        "    T.StructField(\"nps_score\", T.DoubleType(), nullable=True),\n",
        "    T.StructField(\"comment\", T.StringType(), nullable=True),\n",
        "])\n",
        "\n",
        "billing_monthly_schema: Final[T.StructType] = T.StructType([\n",
        "    T.StructField(\"invoice_id\", T.StringType(), nullable=False),\n",
        "    T.StructField(\"org_id\", T.StringType(), nullable=False),\n",
        "    T.StructField(\"month\", T.DateType(), nullable=True),\n",
        "    T.StructField(\"subtotal\", T.DecimalType(10,4), nullable=True),\n",
        "    T.StructField(\"credits\", T.DecimalType(10,4), nullable=True),\n",
        "    T.StructField(\"taxes\", T.DecimalType(10,4), nullable=True),\n",
        "    T.StructField(\"currency\", T.StringType(), nullable=True),\n",
        "    T.StructField(\"exchange_rate_to_usd\", T.DoubleType(), nullable=True),\n",
        "])\n",
        "\n",
        "\n",
        "usage_events_schema: Final[T.StructType] = T.StructType([\n",
        "    T.StructField(\"event_id\", T.StringType(), nullable=False),\n",
        "    T.StructField(\"timestamp\", T.StringType(), nullable=False),\n",
        "    T.StructField(\"org_id\", T.StringType(), nullable=False),\n",
        "    T.StructField(\"resource_id\", T.StringType(), nullable=False),\n",
        "    T.StructField(\"service\", T.StringType(), nullable=True),\n",
        "    T.StructField(\"region\", T.StringType(), nullable=True),\n",
        "    T.StructField(\"metric\", T.StringType(), nullable=True),\n",
        "    T.StructField(\"value\", T.DecimalType(10,4), nullable=True),\n",
        "    T.StructField(\"unit\", T.StringType(), nullable=True),\n",
        "    T.StructField(\"cost_usd_increment\", T.DecimalType(10,4), nullable=True),\n",
        "    T.StructField(\"schema_version\", T.IntegerType(), nullable=True),\n",
        "    T.StructField(\"carbon_kg\", T.DoubleType(), nullable=True),\n",
        "])\n"
      ],
      "metadata": {
        "id": "123EkT026xiE"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# io_utils.py\n",
        "\n",
        "from __future__ import annotations\n",
        "from pathlib import Path\n",
        "from typing import Final\n",
        "from pyspark.sql import DataFrame, SparkSession\n",
        "from pyspark.sql import types as T\n",
        "\n",
        "\n",
        "# --- Helpers de rutas genéricos --- #\n",
        "\n",
        "def zone_path(zone_root: Path, table_name: str) -> Path:\n",
        "    \"\"\"\n",
        "    Devuelve la ruta completa a una tabla dentro de una zona del datalake.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    return (zone_root / table_name).resolve()\n",
        "\n",
        "\n",
        "def add_audit_columns(df: DataFrame) -> DataFrame:\n",
        "    \"\"\"\n",
        "    Enriquece un DataFrame con columnas técnicas de auditoría:\n",
        "    - ingest_ts: Timestamp de ingestión\n",
        "    - source_file: Nombre del archivo origen\n",
        "    \"\"\"\n",
        "    return df.withColumn(\"ingest_ts\", F.current_timestamp()) \\\n",
        "             .withColumn(\"source_file\", F.input_file_name())\n",
        "\n",
        "# --- Lectura de CSV --- #\n",
        "\n",
        "def read_csv(\n",
        "    spark: SparkSession,\n",
        "    path: Path,\n",
        "    schema: T.StructType,\n",
        "    header: bool = True,\n",
        ") -> DataFrame:\n",
        "\n",
        "    #lee CSV con ruta y esquema como param\n",
        "\n",
        "    return (\n",
        "        spark.read\n",
        "        .option(\"header\", str(header).lower())\n",
        "        .schema(schema)\n",
        "        .csv(str(path))\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "def write_parquet(\n",
        "    df: DataFrame,\n",
        "    base_path: Path,\n",
        "    partition_cols: list[str] | None = None,\n",
        "    mode: str = \"overwrite\",\n",
        ") -> None:\n",
        "    #Escribe un  df en formato pparquet\n",
        "\n",
        "    writer = df.write.mode(mode)\n",
        "    if partition_cols:\n",
        "        writer = writer.partitionBy(*partition_cols)\n",
        "    writer.parquet(str(base_path))\n",
        "\n",
        "\n",
        "def read_parquet(spark, base_path, partition_glob: str | None = None):\n",
        "\n",
        "    #lee un Parquet / si no hay partition_glob lee la ruta / si hay artition_glob usa basePath y patron.\n",
        "\n",
        "    base_path = Path(base_path)\n",
        "    base_str = str(base_path)\n",
        "\n",
        "    if partition_glob:\n",
        "        return (\n",
        "            spark.read\n",
        "                 .option(\"basePath\", base_str)\n",
        "                 .parquet(f\"{base_str}/{partition_glob}\")\n",
        "        )\n",
        "\n",
        "    return spark.read.parquet(base_str)\n"
      ],
      "metadata": {
        "id": "bAWzeQUf8ho2"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# audit.py\n",
        "from __future__ import annotations\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.utils import AnalysisException\n",
        "\n",
        "# Esto solo esta en el colab, en el codigo definitivo solo se importa\n",
        "try:\n",
        "    from config import BRONZE_PATH, SILVER_PATH, GOLD_PATH, QUARANTINE_PATH\n",
        "    from io_utils import read_parquet, zone_path\n",
        "except ModuleNotFoundError:\n",
        "    import __main__ as _m\n",
        "    BRONZE_PATH = _m.BRONZE_PATH\n",
        "    SILVER_PATH = _m.SILVER_PATH\n",
        "    GOLD_PATH = _m.GOLD_PATH\n",
        "    QUARANTINE_PATH = _m.QUARANTINE_PATH\n",
        "    read_parquet = _m.read_parquet\n",
        "    zone_path = _m.zone_path\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def audit_bronze_layer(spark: SparkSession, table_name: str, pk_col: str) -> None:\n",
        "\n",
        "    # se chequea vol, uni de PK y el llenado con ingest_ts\n",
        "\n",
        "    print(f\"\\n chequeo Bronze: {table_name}\")\n",
        "\n",
        "    path = zone_path(BRONZE_PATH, table_name)\n",
        "    try:\n",
        "        df = read_parquet(spark, path)\n",
        "    except AnalysisException:\n",
        "        print(f\"[ERR] no se encontro la tabla en {path}\")\n",
        "        return\n",
        "\n",
        "    # Vol\n",
        "    count_total = df.count()\n",
        "\n",
        "    # Uni\n",
        "    count_distinct = df.select(pk_col).distinct().count()\n",
        "    duplicates = count_total - count_distinct\n",
        "\n",
        "    # ingest_ts\n",
        "    if \"ingest_ts\" in df.columns:\n",
        "        null_tech = df.filter(F.col(\"ingest_ts\").isNull()).count()\n",
        "    else:\n",
        "        null_tech = \"falta columna ingest_ts\"\n",
        "\n",
        "    print(f\"registros totales: {count_total}\")\n",
        "    print(f\"duplicados en PK ({pk_col}): {duplicates}\")\n",
        "    print(f\"nulos en ingest_ts: {null_tech}\")\n",
        "\n",
        "    # Resulktado\n",
        "    if duplicates == 0 and (isinstance(null_tech, int) and null_tech == 0):\n",
        "        print(\"Resultado: Todo ok\")\n",
        "    else:\n",
        "        print(\"Resultado: Revisar data por posibles duplicados o falta de metadatos\")\n",
        "\n",
        "\n",
        "def audit_silver_quality(spark: SparkSession) -> None:\n",
        "\n",
        "    #Chequea el resultado del proceso Silver Batch, calculando un ratio entre registros en Silver vs Cuarentena\n",
        "    print(f\"\\n Chequeop Silver:\")\n",
        "\n",
        "    path_good = zone_path(SILVER_PATH, \"usage_events_enriched\")\n",
        "    path_bad  = zone_path(QUARANTINE_PATH, \"usage_events_quarantine\")\n",
        "\n",
        "    # contar buenos\n",
        "    try:\n",
        "        good_df = read_parquet(spark, path_good)\n",
        "        count_good = good_df.count()\n",
        "    except AnalysisException:\n",
        "        count_good = 0\n",
        "\n",
        "    # contar malos\n",
        "    try:\n",
        "        bad_df = read_parquet(spark, path_bad)\n",
        "        count_bad = bad_df.count()\n",
        "        has_bad = True\n",
        "    except AnalysisException:\n",
        "        count_bad = 0\n",
        "        has_bad = False\n",
        "\n",
        "    total = count_good + count_bad\n",
        "    if total == 0:\n",
        "        print(\"[WARN] No hay datos procesados en Silver ni cuarentena.\")\n",
        "        return\n",
        "\n",
        "    bad_ratio = (count_bad / total) * 100\n",
        "\n",
        "    print(f\"Total: {total}\")\n",
        "    print(f\"Aceptados (Silver): {count_good}\")\n",
        "    print(f\"Rechazados (cuarentena): {count_bad} ({bad_ratio:.2f}%)\")\n",
        "\n",
        "    if bad_ratio == 0:\n",
        "        print(\"CALIDAD: PERFECTA (0% Rechazo)\")\n",
        "    elif bad_ratio < 5:\n",
        "        print(\"CALIDAD: ACEPTABLE\")\n",
        "    else:\n",
        "        print(\"CALIDAD: CRÍTICA (>5% Rechazo). Revisar reglas de negocio.\")\n",
        "\n",
        "    if has_bad:\n",
        "        print(\"ejemplo de rechazo:\")\n",
        "        bad_df.select(\"event_id\", \"quarantine_reason\").show(1, truncate=False)\n",
        "\n",
        "\n",
        "def audit_gold_layer(spark: SparkSession, table_name: str) -> None:\n",
        "\n",
        "    #Audita un Mart Gold verificando reglas de negocio para serving layer: Vol, Integridad y KPIs\n",
        "    print(f\"\\n Chequeo Gold: {table_name}\")\n",
        "\n",
        "    path = zone_path(GOLD_PATH, table_name)\n",
        "    try:\n",
        "        # Gold esta particionado\n",
        "        df = read_parquet(spark, path, partition_glob=\"event_date=*\")\n",
        "    except AnalysisException:\n",
        "        print(f\"   [ERR] No se encontró el mart en {path}\")\n",
        "        return\n",
        "\n",
        "    count_total = df.count()\n",
        "\n",
        "    # Regla: Costos Negativos\n",
        "    neg_costs = df.filter(F.col(\"daily_cost_usd\") < 0).count()\n",
        "\n",
        "    print(f\"registros Totales (Agregados): {count_total}\")\n",
        "    print(f\"Costos Negativos detectados: {neg_costs}\")\n",
        "\n",
        "    if neg_costs == 0:\n",
        "        print(\"todo listo para serving layer\")\n",
        "    else:\n",
        "        print(\"error: data corrupta en Gold\")\n",
        "\n",
        "def audit_quarantine(spark: SparkSession):\n",
        "    print(f\"\\nCalidad de Datos (Silver Batch)\")\n",
        "\n",
        "    path_good = zone_path(SILVER_PATH, \"usage_events_enriched\")\n",
        "    path_bad  = zone_path(QUARANTINE_PATH, \"usage_events_quarantine\")\n",
        "\n",
        "    # conteo de datos buenos\n",
        "    try:\n",
        "        good_df = read_parquet(spark, path_good)\n",
        "        count_good = good_df.count()\n",
        "    except AnalysisException:\n",
        "        count_good = 0\n",
        "        print(\"[WARN] No hay data en Silver.\")\n",
        "\n",
        "    # conteo  malos\n",
        "    try:\n",
        "        bad_df = read_parquet(spark, path_bad)\n",
        "        count_bad = bad_df.count()\n",
        "        has_bad_data = True\n",
        "    except AnalysisException:\n",
        "        count_bad = 0\n",
        "        has_bad_data = False\n",
        "        print(\"[INFO] No hay data en cuarentena\")\n",
        "\n",
        "    # ratio\n",
        "    total = count_good + count_bad\n",
        "    if total == 0:\n",
        "        print(\"[ERR] No hay data procesada\")\n",
        "        return\n",
        "\n",
        "    bad_ratio = (count_bad / total) * 100\n",
        "\n",
        "    print(f\"\\n Estadisticas:\")\n",
        "    print(f\"Total Procesado: {total}\")\n",
        "    print(f\"Aceptados (Silver): {count_good} ({(100 - bad_ratio):.2f}%)\")\n",
        "    print(f\"Rechazados (Quarantine): {count_bad} ({bad_ratio:.2f}%)\")\n",
        "\n",
        "    print(\"\\nresultado:\")\n",
        "\n",
        "    if bad_ratio == 0:\n",
        "        print(\"Satifactorio - sin datos rechazados\")\n",
        "    elif bad_ratio < 5:\n",
        "        print(\"Aceptable- rechazo  bajo y esperado.\")\n",
        "    else:\n",
        "        print(\"malo -demasiada data rechazada (>5%)\")\n",
        "\n",
        "    # muestra errores\n",
        "    if has_bad_data:\n",
        "        print(\"\\n Muestra de registros en Cuarentena (top 5):\")\n",
        "        cols_to_show = [\"event_id\", \"cost_usd_increment\", \"org_id\", \"quarantine_reason\"]\n",
        "        actual_cols = [c for c in cols_to_show if c in bad_df.columns]\n",
        "        bad_df.select(*actual_cols).show(5, truncate=False)\n",
        "\n",
        "\n",
        "def audit_speed_layer_results(spark: SparkSession):\n",
        "\n",
        "    #aca se chequea que la Speed Layer haya persistido datos en disco.\n",
        "    #Se ejecuta despues de parar el stream.\n",
        "\n",
        "    print(f\"\\n cheque de Speed Layer ---\")\n",
        "    path = zone_path(GOLD_PATH, \"org_daily_usage_by_service_speed\")\n",
        "\n",
        "    try:\n",
        "        df = read_parquet(spark, path, partition_glob=\"*\")\n",
        "        total_rows = df.count()\n",
        "\n",
        "        print(f\"Ruta: {path}, Total acumulado en disco: {total_rows}\")\n",
        "\n",
        "        if total_rows > 0:\n",
        "          print(\"funcionando todo OK (Datos persistidos correctamente)\")\n",
        "          df.show(3, truncate=False)\n",
        "        else:\n",
        "          print(\"vacio - dejar el stream corriendo mas tiempo\")\n",
        "    except Exception as e:\n",
        "       print(f\"[ERR] No se pudo leer la Speed Layer: {e}\")\n"
      ],
      "metadata": {
        "id": "-HWtT_Wua_KS"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# bronze_batch.py\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "from pathlib import Path\n",
        "from typing import Optional\n",
        "\n",
        "from pyspark.sql import DataFrame, SparkSession\n",
        "\n",
        "\n",
        "try:\n",
        "    from config import LANDING_PATH, BRONZE_PATH\n",
        "\n",
        "except ModuleNotFoundError:\n",
        "    import __main__ as _m\n",
        "    try:\n",
        "        LANDING_PATH = _m.LANDING_PATH\n",
        "        BRONZE_PATH = _m.BRONZE_PATH\n",
        "    except AttributeError as exc:\n",
        "        raise RuntimeError(\n",
        "            \"No se pudo importar config, hay que corre primero la celda 'config.py'.\"\n",
        "        ) from exc\n",
        "\n",
        "\n",
        "# importacion de squemas\n",
        "\n",
        "try:\n",
        "    from schemas import (\n",
        "        customers_orgs_schema,\n",
        "        users_schema,\n",
        "        resources_schema,\n",
        "        support_tickets_schema,\n",
        "        marketing_touches_schema,\n",
        "        nps_surveys_schema,\n",
        "        billing_monthly_schema,\n",
        "    )\n",
        "except ModuleNotFoundError:\n",
        "    import __main__ as _m  # type: ignore[import]\n",
        "    try:\n",
        "        customers_orgs_schema = _m.customers_orgs_schema\n",
        "        users_schema = _m.users_schema\n",
        "        resources_schema = _m.resources_schema\n",
        "        support_tickets_schema = _m.support_tickets_schema\n",
        "        marketing_touches_schema = _m.marketing_touches_schema\n",
        "        nps_surveys_schema = _m.nps_surveys_schema\n",
        "        billing_monthly_schema = _m.billing_monthly_schema\n",
        "    except AttributeError as exc:\n",
        "        raise RuntimeError(\n",
        "            \"No se pudo importar schemas, hayque correr primero la celda 'schemas.py'\"\n",
        "        ) from exc\n",
        "\n",
        "\n",
        "#  importacion de todo lo qe es IO desde io_utils\n",
        "\n",
        "try:\n",
        "    from io_utils import read_csv, write_parquet, zone_path, add_audit_columns\n",
        "except ModuleNotFoundError:\n",
        "    import __main__ as _m\n",
        "    try:\n",
        "        read_csv = _m.read_csv\n",
        "        write_parquet = _m.write_parquet\n",
        "        zone_path = _m.zone_path\n",
        "    except AttributeError as exc:\n",
        "        raise RuntimeError(\n",
        "            \"No se pudo importar io_utils, hay que correr antes primero la celda 'io_utils.py'.\"\n",
        "        ) from exc\n",
        "\n",
        "\n",
        "# helper interno para leer .csv\n",
        "\n",
        "def _read_landing_csv(\n",
        "    spark: SparkSession,\n",
        "    file_name: str,\n",
        "    schema,\n",
        ") -> Optional[DataFrame]:\n",
        "\n",
        "    csv_path = LANDING_PATH / file_name\n",
        "    if not csv_path.exists():\n",
        "        print(f\"[WARN] CSV no encontrado en landing: {csv_path}\")\n",
        "        return None\n",
        "\n",
        "    print(f\"[INFO] Leyendo {csv_path}\")\n",
        "    return read_csv(spark, csv_path, schema)\n",
        "\n",
        "\n",
        "# Ingesta\n",
        "\n",
        "def ingest_customers_orgs_to_bronze(spark: SparkSession) -> None:\n",
        "    df = _read_landing_csv(spark, \"customers_orgs.csv\", customers_orgs_schema)\n",
        "    if df is None: return\n",
        "    df = add_audit_columns(df)\n",
        "    dest = zone_path(BRONZE_PATH, \"customers_orgs\")\n",
        "    write_parquet(df, dest, partition_cols=[\"hq_region\"])\n",
        "    print(f\"[OK] Bronze customers_orgs -> {dest}\")\n",
        "\n",
        "\n",
        "def ingest_users_to_bronze(spark: SparkSession) -> None:\n",
        "    df = _read_landing_csv(spark, \"users.csv\", users_schema)\n",
        "    if df is None:\n",
        "        return\n",
        "    df = add_audit_columns(df)\n",
        "    dest = zone_path(BRONZE_PATH, \"users\")\n",
        "    write_parquet(df, dest, partition_cols=[\"role\"])\n",
        "    print(f\"[OK] Bronze users -> {dest}\")\n",
        "\n",
        "\n",
        "def ingest_resources_to_bronze(spark: SparkSession) -> None:\n",
        "    df = _read_landing_csv(spark, \"resources.csv\", resources_schema)\n",
        "    if df is None:\n",
        "        return\n",
        "    df = add_audit_columns(df)\n",
        "    dest = zone_path(BRONZE_PATH, \"resources\")\n",
        "    write_parquet(df, dest, partition_cols=[\"region\"])\n",
        "    print(f\"[OK] Bronze resources -> {dest}\")\n",
        "\n",
        "\n",
        "def ingest_support_tickets_to_bronze(spark: SparkSession) -> None:\n",
        "    df = _read_landing_csv(spark, \"support_tickets.csv\", support_tickets_schema)\n",
        "    if df is None:\n",
        "        return\n",
        "    df = add_audit_columns(df)\n",
        "    dest = zone_path(BRONZE_PATH, \"support_tickets\")\n",
        "    write_parquet(df, dest, partition_cols=[\"severity\"])\n",
        "    print(f\"[OK] Bronze support_tickets -> {dest}\")\n",
        "\n",
        "\n",
        "def ingest_marketing_touches_to_bronze(spark: SparkSession) -> None:\n",
        "    df = _read_landing_csv(spark, \"marketing_touches.csv\", marketing_touches_schema)\n",
        "    if df is None:\n",
        "        return\n",
        "    df = add_audit_columns(df)\n",
        "    dest = zone_path(BRONZE_PATH, \"marketing_touches\")\n",
        "    write_parquet(df, dest, partition_cols=[\"channel\"])\n",
        "    print(f\"[OK] Bronze marketing_touches -> {dest}\")\n",
        "\n",
        "\n",
        "def ingest_nps_surveys_to_bronze(spark: SparkSession) -> None:\n",
        "    df = _read_landing_csv(spark, \"nps_surveys.csv\", nps_surveys_schema)\n",
        "    if df is None:\n",
        "        return\n",
        "    df = add_audit_columns(df)\n",
        "    dest = zone_path(BRONZE_PATH, \"nps_surveys\")\n",
        "    write_parquet(df, dest, partition_cols=[\"survey_date\"])\n",
        "    print(f\"[OK] Bronze nps_surveys -> {dest}\")\n",
        "\n",
        "\n",
        "def ingest_billing_monthly_to_bronze(spark: SparkSession) -> None:\n",
        "    df = _read_landing_csv(spark, \"billing_monthly.csv\", billing_monthly_schema)\n",
        "    if df is None:\n",
        "        return\n",
        "    df = add_audit_columns(df)\n",
        "    dest = zone_path(BRONZE_PATH, \"billing_monthly\")\n",
        "    write_parquet(df, dest, partition_cols=[\"month\"])\n",
        "    print(f\"[OK] Bronze billing_monthly -> {dest}\")\n",
        "\n",
        "\n",
        "# orrquestador de Bronze batch\n",
        "\n",
        "def run_bronze_batch(spark: SparkSession) -> None:\n",
        "    ingest_customers_orgs_to_bronze(spark)\n",
        "    ingest_users_to_bronze(spark)\n",
        "    ingest_resources_to_bronze(spark)\n",
        "    ingest_support_tickets_to_bronze(spark)\n",
        "    ingest_marketing_touches_to_bronze(spark)\n",
        "    ingest_nps_surveys_to_bronze(spark)\n",
        "    ingest_billing_monthly_to_bronze(spark)"
      ],
      "metadata": {
        "id": "kY96k5ph9koG"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "-main bronze-"
      ],
      "metadata": {
        "id": "qkM2ckNzAU6L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ensure_dirs()\n",
        "unpack_raw_dataset()\n",
        "spark = create_spark()\n",
        "run_bronze_batch(spark)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "leiPDPIlARet",
        "outputId": "e92fce8f-acc4-4c45-e3fe-ee51b8bf0fec"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Leyendo /content/datalake/landing/customers_orgs.csv\n",
            "[OK] Bronze customers_orgs -> /content/datalake/bronze/customers_orgs\n",
            "[INFO] Leyendo /content/datalake/landing/users.csv\n",
            "[OK] Bronze users -> /content/datalake/bronze/users\n",
            "[INFO] Leyendo /content/datalake/landing/resources.csv\n",
            "[OK] Bronze resources -> /content/datalake/bronze/resources\n",
            "[INFO] Leyendo /content/datalake/landing/support_tickets.csv\n",
            "[OK] Bronze support_tickets -> /content/datalake/bronze/support_tickets\n",
            "[INFO] Leyendo /content/datalake/landing/marketing_touches.csv\n",
            "[OK] Bronze marketing_touches -> /content/datalake/bronze/marketing_touches\n",
            "[INFO] Leyendo /content/datalake/landing/nps_surveys.csv\n",
            "[OK] Bronze nps_surveys -> /content/datalake/bronze/nps_surveys\n",
            "[INFO] Leyendo /content/datalake/landing/billing_monthly.csv\n",
            "[OK] Bronze billing_monthly -> /content/datalake/bronze/billing_monthly\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " # bronze_stream.py\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "from typing import Optional\n",
        "\n",
        "from pyspark.sql import DataFrame, SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "\n",
        "try:\n",
        "    from config import LANDING_PATH, BRONZE_PATH\n",
        "except ModuleNotFoundError:\n",
        "    import __main__ as _m\n",
        "    try:\n",
        "        LANDING_PATH = _m.LANDING_PATH\n",
        "        BRONZE_PATH = _m.BRONZE_PATH\n",
        "    except AttributeError as exc:\n",
        "        raise RuntimeError(\n",
        "            \"No se pudo importar config, hay que correr primero la celda 'config.py'.\"\n",
        "        ) from exc\n",
        "\n",
        "\n",
        "try:\n",
        "    from schemas import usage_events_schema\n",
        "except ModuleNotFoundError:\n",
        "    import __main__ as _m\n",
        "    try:\n",
        "        usage_events_schema = _m.usage_events_schema\n",
        "    except AttributeError as exc:\n",
        "        raise RuntimeError(\n",
        "            \"No se pudo importar schemas, hay que correr primero la celda 'schemas.py'.\"\n",
        "        ) from exc\n",
        "\n",
        "\n",
        "\n",
        "try:\n",
        "    from io_utils import zone_path\n",
        "except ModuleNotFoundError:\n",
        "    import __main__ as _m\n",
        "    try:\n",
        "        zone_path = _m.zone_path\n",
        "    except AttributeError as exc:\n",
        "        raise RuntimeError(\n",
        "            \"No se pudo importar io_utils hay qeu ejecutar primero la celda 'io_utils.py'.\"\n",
        "        ) from exc\n",
        "\n",
        "\n",
        "# -creacion de DF de streaming\n",
        "\n",
        "def create_usage_events_stream(spark: SparkSession) -> DataFrame:\n",
        "\n",
        "    src_dir = LANDING_PATH / \"usage_events_stream\"\n",
        "\n",
        "    return (\n",
        "        spark.readStream\n",
        "        .schema(usage_events_schema)\n",
        "        .option(\"maxFilesPerTrigger\", 1)\n",
        "        .json(str(src_dir))\n",
        "    )\n",
        "\n",
        "\n",
        "def transform_usage_events_bronze(df_stream: DataFrame) -> DataFrame:\n",
        "\n",
        "    # Transformaciones :'timestamp' a 'event_ts', 'event_date' (date),  watermark y dedupe por event_id\n",
        "\n",
        "    df = (\n",
        "        df_stream\n",
        "        .withColumn(\"event_ts\", F.to_timestamp(\"timestamp\"))\n",
        "        .withColumn(\"event_date\", F.to_date(\"event_ts\"))\n",
        "    )\n",
        "\n",
        "    df = (\n",
        "      df\n",
        "      .withWatermark(\"event_ts\", \"1 day\")\n",
        "      .dropDuplicates([\"event_id\"])\n",
        "    )\n",
        "\n",
        "    return df\n",
        "\n",
        "  # Arranca el streaming desde usage_events_stream en Landing\n",
        "\n",
        "def start_usage_events_to_bronze(spark: SparkSession):\n",
        "\n",
        "    df_stream = create_usage_events_stream(spark)\n",
        "    df_bronze = transform_usage_events_bronze(df_stream)\n",
        "\n",
        "    dest_path = zone_path(BRONZE_PATH, \"usage_events\")\n",
        "    checkpoint_path = BRONZE_PATH / \"_checkpoints\" / \"usage_events\"\n",
        "\n",
        "    query = (\n",
        "        df_bronze\n",
        "        .writeStream\n",
        "        .format(\"parquet\")\n",
        "        .option(\"checkpointLocation\", str(checkpoint_path))\n",
        "        .option(\"path\", str(dest_path))\n",
        "        .partitionBy(\"event_date\")\n",
        "        .outputMode(\"append\")\n",
        "        .start()\n",
        "    )\n",
        "\n",
        "    print(f\"[INFO] Streaming usage_events -> {dest_path}\")\n",
        "    print(f\"[INFO] Checkpoints en {checkpoint_path}\")\n",
        "    return query"
      ],
      "metadata": {
        "id": "rQ8EQ1tKDDEV"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# esto es para correr en notebook\n",
        "import time\n",
        "\n",
        "# runner de streaming usage_events (para notebook)\n",
        "query_usage = start_usage_events_to_bronze(spark)\n",
        "\n",
        "print(\"[INFO] corriendo el stream por 20 segundos...\")\n",
        "time.sleep(20)\n",
        "\n",
        "print(\"[INFO] parando stream...\")\n",
        "query_usage.stop()\n",
        "print(\"[INFO] Stream parado.\")"
      ],
      "metadata": {
        "id": "Hbx3Qgvqk129",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5365c290-28e7-46ff-d597-7bcfa77a45ca"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Streaming usage_events -> /content/datalake/bronze/usage_events\n",
            "[INFO] Checkpoints en /content/datalake/bronze/_checkpoints/usage_events\n",
            "[INFO] corriendo el stream por 20 segundos...\n",
            "[INFO] parando stream...\n",
            "[INFO] Stream parado.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# silver.py\n",
        "\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "from pyspark.sql import DataFrame, SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "# parche de imports para colab de nuevo, esto en el PY final no va a estar\n",
        "try:\n",
        "    from config import BRONZE_PATH, SILVER_PATH, QUARANTINE_PATH\n",
        "except ModuleNotFoundError:\n",
        "    import __main__ as _m\n",
        "    BRONZE_PATH = _m.BRONZE_PATH\n",
        "    SILVER_PATH = _m.SILVER_PATH\n",
        "    QUARANTINE_PATH = _m.QUARANTINE_PATH\n",
        "\n",
        "try:\n",
        "    from io_utils import read_parquet, write_parquet, zone_path\n",
        "except ModuleNotFoundError:\n",
        "    import __main__ as _m\n",
        "    read_parquet = _m.read_parquet\n",
        "    write_parquet = _m.write_parquet\n",
        "    zone_path = _m.zone_path\n",
        "\n",
        "\n",
        "\n",
        "def read_bronze_usage_events(spark: SparkSession) -> DataFrame:\n",
        "    return read_parquet(spark, zone_path(BRONZE_PATH, \"usage_events\"), partition_glob=\"event_date=*\")\n",
        "\n",
        "def read_bronze_customers_orgs(spark: SparkSession) -> DataFrame:\n",
        "    return read_parquet(spark, zone_path(BRONZE_PATH, \"customers_orgs\"))\n",
        "\n",
        "# -impieza, Joins y cuarentena\n",
        "\n",
        "def run_silver_batch(spark: SparkSession) -> None:\n",
        "\n",
        "    print(\"[INFO] Iniciando Silver...\")\n",
        "\n",
        "    usage_df = read_bronze_usage_events(spark)\n",
        "    orgs_df = read_bronze_customers_orgs(spark)\n",
        "\n",
        "    orgs_sel = orgs_df.select(\n",
        "        \"org_id\", \"org_name\", \"hq_region\", \"plan_tier\", \"is_enterprise\"\n",
        "    )\n",
        "\n",
        "    # Join broadcast) // se usa broadcast porque orgs es chica comparada con eventos\n",
        "    enriched_df = usage_df.join(F.broadcast(orgs_sel), on=\"org_id\", how=\"left\")\n",
        "\n",
        "    # Reglas:\n",
        "    # 1: El costo no puede ser negativo (permitimos 0 o mayor, o -0.00...1 errores de float y decimal, pero defino corte en -0.01 para mayor seguridad)\n",
        "    # 2: tiene tener org_id (el join lo mantiene, pero se valida que no sea nulo si era inner logic)\n",
        "    dq_condition = (F.col(\"cost_usd_increment\") >= -0.01) & (F.col(\"org_id\").isNotNull())\n",
        "\n",
        "    # split\n",
        "    good_df = enriched_df.filter(dq_condition)\n",
        "    bad_df = enriched_df.filter(~dq_condition)\n",
        "\n",
        "    if not bad_df.rdd.isEmpty():\n",
        "        # A. Preparar datos fallidos actuales\n",
        "        bad_df = bad_df.withColumn(\"quarantine_reason\", F.lit(\"cost_negative_or_null_org\"))\n",
        "\n",
        "        quarantine_dest = zone_path(QUARANTINE_PATH, \"usage_events_quarantine\")\n",
        "\n",
        "        #  se verifica si ya existe para no duplicar\n",
        "        try:\n",
        "            existing_quarantine = read_parquet(spark, quarantine_dest)\n",
        "\n",
        "            # C. aca hago el left anti join para no tener dupes en cuarentena\n",
        "            unique_bad_df = bad_df.join(\n",
        "                existing_quarantine,\n",
        "                on=\"event_id\",\n",
        "                how=\"left_anti\"\n",
        "            )\n",
        "\n",
        "            new_errors_count = unique_bad_df.count()\n",
        "            if new_errors_count > 0:\n",
        "                print(f\"[WARN] Nuevos registros invalidos detectados: {new_errors_count}\")\n",
        "                write_parquet(unique_bad_df, quarantine_dest, mode=\"append\")\n",
        "            else:\n",
        "                print(f\"[INFO] Errores detectados ya existian en cuarentena\")\n",
        "\n",
        "        except Exception:\n",
        "            # si no hay archivo, se crea\n",
        "            print(f\"[WARN] Creando cuarentena por primera vez\")\n",
        "            write_parquet(bad_df, quarantine_dest, mode=\"append\")\n",
        "\n",
        "    # escritura de Silver limpio\n",
        "    silver_dest = zone_path(SILVER_PATH, \"usage_events_enriched\")\n",
        "    good_df = good_df.withColumnRenamed(\"service\", \"service_name\")\n",
        "\n",
        "    write_parquet(\n",
        "        good_df,\n",
        "        silver_dest,\n",
        "        partition_cols=[\"event_date\"],\n",
        "        mode=\"overwrite\"\n",
        "    )\n",
        "    print(f\"[OK] Silver Batch completado, todo ok -> {silver_dest}\")"
      ],
      "metadata": {
        "id": "tUJAJo1OSDp1"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# gold.py\n",
        "\n",
        "from __future__ import annotations\n",
        "from pyspark.sql import DataFrame, SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "# parche de imports para colab de nuevo, esto en el PY final no va a estar\n",
        "try:\n",
        "    from config import SILVER_PATH, GOLD_PATH, BRONZE_PATH\n",
        "except ModuleNotFoundError:\n",
        "    import __main__ as _m\n",
        "    SILVER_PATH = _m.SILVER_PATH\n",
        "    GOLD_PATH = _m.GOLD_PATH\n",
        "    BRONZE_PATH = _m.BRONZE_PATH\n",
        "\n",
        "try:\n",
        "    from io_utils import read_parquet, write_parquet, zone_path\n",
        "except ModuleNotFoundError:\n",
        "    import __main__ as _m\n",
        "    read_parquet = _m.read_parquet\n",
        "    write_parquet = _m.write_parquet\n",
        "    zone_path = _m.zone_path\n",
        "\n",
        "try:\n",
        "    from bronze_stream import create_usage_events_stream, transform_usage_events_bronze\n",
        "except ModuleNotFoundError:\n",
        "    import __main__ as _m\n",
        "    create_usage_events_stream = _m.create_usage_events_stream\n",
        "    transform_usage_events_bronze = _m.transform_usage_events_bronze\n",
        "\n",
        "\n",
        "# Gold de Batch, aca se calculan los finops\n",
        "\n",
        "def build_gold_finops_mart(spark: SparkSession) -> DataFrame:\n",
        "  # se crea el Mart de FinOps agregando datos desde Silver limpuio considerando org_id, service_name, event_date.\n",
        "\n",
        "\n",
        "    silver_path = zone_path(SILVER_PATH, \"usage_events_enriched\")\n",
        "    silver_df = read_parquet(spark, silver_path)\n",
        "\n",
        "    # agregaciones\n",
        "    aggregated_df = (\n",
        "        silver_df\n",
        "        .groupBy(\"org_id\", \"org_name\", \"service_name\", \"event_date\", \"hq_region\", \"plan_tier\")\n",
        "        .agg(\n",
        "            F.sum(\"cost_usd_increment\").alias(\"daily_cost_usd\"),\n",
        "            F.sum(\n",
        "                F.when(F.col(\"metric\") == \"requests\", F.col(\"value\")).otherwise(0.0)\n",
        "            ).alias(\"daily_requests\"),\n",
        "            F.sum(\"carbon_kg\").alias(\"daily_carbon_kg\")\n",
        "        )\n",
        "    )\n",
        "\n",
        "    #KPIs\n",
        "    gold_df = (\n",
        "        aggregated_df\n",
        "        .withColumn(\n",
        "            \"cost_per_request\",\n",
        "            F.when(F.col(\"daily_requests\") > 0,\n",
        "                   F.col(\"daily_cost_usd\") / F.col(\"daily_requests\")).otherwise(None)\n",
        "        )\n",
        "        .withColumn(\n",
        "            \"carbon_per_dollar\",\n",
        "            F.when(F.col(\"daily_cost_usd\") > 0,\n",
        "                   F.col(\"daily_carbon_kg\") / F.col(\"daily_cost_usd\")).otherwise(None)\n",
        "        )\n",
        "    )\n",
        "\n",
        "    return gold_df\n",
        "\n",
        "def run_gold_batch(spark: SparkSession) -> None:\n",
        "    df = build_gold_finops_mart(spark)\n",
        "    dest = zone_path(GOLD_PATH, \"org_daily_usage_by_service\")\n",
        "    write_parquet(df, dest, partition_cols=[\"event_date\"], mode=\"overwrite\")\n",
        "    print(f\"[OK] Gold Batch (FinOps) -> {dest}\")\n",
        "\n",
        "\n",
        "# SPEED GOLD: Streaming Directo a Gold\n",
        "\n",
        "\n",
        "def start_gold_speed_stream(spark: SparkSession):\n",
        "\n",
        "    # Speed Layer: Lee stream yvuelca a Gold\n",
        "    # se usa cache de  Orgs para evitar I/O repetitivo y Coalesce(1) para evitar el problema de tener muchos archivos chicos en Gold.\n",
        "\n",
        "    print(\"[INFO] Comenzando Speed Layer...\")\n",
        "\n",
        "    # Stream\n",
        "    raw_stream = create_usage_events_stream(spark)\n",
        "    stream_bronze = transform_usage_events_bronze(raw_stream)\n",
        "\n",
        "    #  cacheo de Orgs\n",
        "\n",
        "    orgs_df = read_parquet(spark, zone_path(BRONZE_PATH, \"customers_orgs\"))\n",
        "    orgs_sel = orgs_df.select(\"org_id\", \"org_name\", \"hq_region\", \"plan_tier\")\n",
        "    orgs_sel.cache()\n",
        "    print(f\"[INFO] Dimensión Organizaciones cacheada: {orgs_sel.count()} registros.\")\n",
        "\n",
        "\n",
        "    dest_speed = zone_path(GOLD_PATH, \"org_daily_usage_by_service_speed\")\n",
        "\n",
        "    def process_microbatch(batch_df: DataFrame, batch_id: int):\n",
        "\n",
        "        if batch_df.rdd.isEmpty():\n",
        "            return\n",
        "\n",
        "        # metricas\n",
        "        input_count = batch_df.count()\n",
        "\n",
        "        # procesado\n",
        "        enriched = batch_df.join(F.broadcast(orgs_sel), on=\"org_id\", how=\"left\")\n",
        "\n",
        "        # data quality\n",
        "        valid_stream = enriched.filter(F.col(\"cost_usd_increment\") >= -0.01)\n",
        "        valid_count = valid_stream.count()\n",
        "        dropped_count = input_count - valid_count\n",
        "\n",
        "        # aregaciones\n",
        "        agg_batch = (\n",
        "            valid_stream\n",
        "            .groupBy(\"org_id\", \"org_name\", \"service\", \"event_date\")\n",
        "            .agg(\n",
        "                F.sum(\"cost_usd_increment\").alias(\"daily_cost_usd\"),\n",
        "                F.sum(F.when(F.col(\"metric\") == \"requests\", F.col(\"value\")).otherwise(0)).alias(\"daily_requests\"),\n",
        "                F.sum(\"carbon_kg\").alias(\"daily_carbon_kg\")\n",
        "            )\n",
        "            .withColumnRenamed(\"service\", \"service_name\")\n",
        "        )\n",
        "\n",
        "        # Append\n",
        "        (\n",
        "            agg_batch\n",
        "            .coalesce(1)\n",
        "            .write\n",
        "            .mode(\"append\")\n",
        "            .partitionBy(\"event_date\")\n",
        "            .parquet(str(dest_speed))\n",
        "        )\n",
        "\n",
        "        #  Logde quality para monitoreo ---\n",
        "\n",
        "        print(f\"[STREAM {batch_id}] Reporte \")\n",
        "        print(f\" Input: {input_count} eventos\")\n",
        "        print(f\"Validos: {valid_count}\")\n",
        "        if dropped_count > 0:\n",
        "            print(f\"Dropped (Cost < -0.01): {dropped_count} ({(dropped_count/input_count)*100:.1f}%)\")\n",
        "        print(f\"todo pasado a Gold de Speed layer\")\n",
        "\n",
        "    # Arranca Stream con outputMode(\"update\") para permitir agregacionesy con  foreachBatch manejando la salida final\n",
        "    query = (\n",
        "        stream_bronze\n",
        "        .writeStream\n",
        "        .foreachBatch(process_microbatch)\n",
        "        .outputMode(\"update\")\n",
        "        .trigger(processingTime=\"5 seconds\") # trigger para no saturar\n",
        "        .start()\n",
        "    )\n",
        "\n",
        "    print(f\"[INFO] Streaming ejecutandose -> {dest_speed}\")\n",
        "    return query"
      ],
      "metadata": {
        "id": "4wEDN8i8TvOF"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cassandra_loader.py\n",
        "\n",
        "def upload_gold_to_cassandra(spark: SparkSession):\n",
        "\n",
        "    # Lee la tabla Gold y la carga en Cassandra\n",
        "\n",
        "    print(\"\\n[SERVING] Iniciando carga a Cassandra...\")\n",
        "\n",
        "    # Esquema (DDL)\n",
        "    try:\n",
        "        session = get_cassandra_session()\n",
        "        create_schema(session)\n",
        "        session.shutdown()\n",
        "    except Exception as e:\n",
        "        print(f\"[ERR] Error conectando a Cassandra: {e}\")\n",
        "        return\n",
        "\n",
        "\n",
        "    gold_df = read_parquet(spark, zone_path(GOLD_PATH, \"org_daily_usage_by_service\"), partition_glob=\"event_date=*\")\n",
        "\n",
        "    # convierto a python con el driver\n",
        "\n",
        "    print(f\"[SERVING] Leyendo {gold_df.count()} filas de Gold...\")\n",
        "    rows = gold_df.collect()\n",
        "    data_to_insert = [row.asDict() for row in rows]\n",
        "\n",
        "    insert_batch_to_cassandra(data_to_insert)\n",
        "    print(f\"[SERVING] Carga completada. {len(data_to_insert)} registros insertados.\")\n",
        "\n",
        "# --- PARA EL STREAMING (Opcional / Puntos Extra) ---\n",
        "def write_stream_to_cassandra(batch_df, batch_id):\n",
        "    \"\"\"\n",
        "    Función para usar en .foreachBatch del Streaming.\n",
        "    \"\"\"\n",
        "    if batch_df.rdd.isEmpty(): return\n",
        "\n",
        "    # Convertir a lista de dicts\n",
        "    rows = batch_df.collect()\n",
        "    data = [row.asDict() for row in rows]\n",
        "\n",
        "    # Insertar (Upsert por naturaleza de Cassandra)\n",
        "    insert_batch_to_cassandra(data)\n",
        "    print(f\"[CASSANDRA STREAM] Batch {batch_id} cargado ({len(data)} filas).\")"
      ],
      "metadata": {
        "id": "o5KrZ9GVoqnu"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Demo de Batch Layer → Gold"
      ],
      "metadata": {
        "id": "YnGKPv-va_h4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Demo Batch Layer\")\n",
        "\n",
        "run_bronze_batch(spark)\n",
        "audit_bronze_layer(spark, \"customers_orgs\", pk_col=\"org_id\")\n",
        "\n",
        "run_silver_batch(spark)\n",
        "audit_silver_quality(spark)\n",
        "audit_quarantine(spark)\n",
        "\n",
        "run_gold_batch(spark)\n",
        "audit_gold_layer(spark, \"org_daily_usage_by_service\")\n",
        "\n",
        "\n",
        "print(\"\\nFin demo Batch Layer (Bronze CSV → Silver → Gold)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af802ae8-d700-44fe-a6c8-11c11863c9a6",
        "id": "7BBc-2f1gNkG"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Demo Batch Layer\n",
            "[INFO] Leyendo /content/datalake/landing/customers_orgs.csv\n",
            "[OK] Bronze customers_orgs -> /content/datalake/bronze/customers_orgs\n",
            "[INFO] Leyendo /content/datalake/landing/users.csv\n",
            "[OK] Bronze users -> /content/datalake/bronze/users\n",
            "[INFO] Leyendo /content/datalake/landing/resources.csv\n",
            "[OK] Bronze resources -> /content/datalake/bronze/resources\n",
            "[INFO] Leyendo /content/datalake/landing/support_tickets.csv\n",
            "[OK] Bronze support_tickets -> /content/datalake/bronze/support_tickets\n",
            "[INFO] Leyendo /content/datalake/landing/marketing_touches.csv\n",
            "[OK] Bronze marketing_touches -> /content/datalake/bronze/marketing_touches\n",
            "[INFO] Leyendo /content/datalake/landing/nps_surveys.csv\n",
            "[OK] Bronze nps_surveys -> /content/datalake/bronze/nps_surveys\n",
            "[INFO] Leyendo /content/datalake/landing/billing_monthly.csv\n",
            "[OK] Bronze billing_monthly -> /content/datalake/bronze/billing_monthly\n",
            "\n",
            " chequeo Bronze: customers_orgs\n",
            "registros totales: 80\n",
            "duplicados en PK (org_id): 0\n",
            "nulos en ingest_ts: 0\n",
            "Resultado: Todo ok\n",
            "[INFO] Iniciando Silver...\n",
            "[INFO] Errores detectados ya existian en cuarentena\n",
            "[OK] Silver Batch completado, todo ok -> /content/datalake/silver/usage_events_enriched\n",
            "\n",
            " Chequeop Silver:\n",
            "Total: 933\n",
            "Aceptados (Silver): 930\n",
            "Rechazados (cuarentena): 3 (0.32%)\n",
            "CALIDAD: ACEPTABLE\n",
            "ejemplo de rechazo:\n",
            "+----------------+-------------------------+\n",
            "|event_id        |quarantine_reason        |\n",
            "+----------------+-------------------------+\n",
            "|evt_faubjbtabmwl|cost_negative_or_null_org|\n",
            "+----------------+-------------------------+\n",
            "only showing top 1 row\n",
            "\n",
            "\n",
            "Calidad de Datos (Silver Batch)\n",
            "\n",
            " Estadisticas:\n",
            "Total Procesado: 933\n",
            "Aceptados (Silver): 930 (99.68%)\n",
            "Rechazados (Quarantine): 3 (0.32%)\n",
            "\n",
            "resultado:\n",
            "Aceptable- rechazo  bajo y esperado.\n",
            "\n",
            " Muestra de registros en Cuarentena (top 5):\n",
            "+----------------+------------------+------------+-------------------------+\n",
            "|event_id        |cost_usd_increment|org_id      |quarantine_reason        |\n",
            "+----------------+------------------+------------+-------------------------+\n",
            "|evt_faubjbtabmwl|-0.0602           |org_cvs4f8cg|cost_negative_or_null_org|\n",
            "|evt_qniow8ymxwd6|-0.2446           |org_i7p5tb94|cost_negative_or_null_org|\n",
            "|evt_bbmth9hzpa6e|-0.3656           |org_n9j2qp89|cost_negative_or_null_org|\n",
            "+----------------+------------------+------------+-------------------------+\n",
            "\n",
            "[OK] Gold Batch (FinOps) -> /content/datalake/gold/org_daily_usage_by_service\n",
            "\n",
            " Chequeo Gold: org_daily_usage_by_service\n",
            "registros Totales (Agregados): 816\n",
            "Costos Negativos detectados: 0\n",
            "todo listo para serving layer\n",
            "\n",
            "Fin demo Batch Layer (Bronze CSV → Silver → Gold)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Demo de Speed Layer → Gold\n",
        "\n",
        "En un despliegue real, el pipeline de streaming se ejecutaría como servicio/orquestación aparte"
      ],
      "metadata": {
        "id": "szSUhws8Zj4p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DEMO Speed → Gold\n",
        "import time\n",
        "\n",
        "query_speed_gold = start_gold_speed_stream(spark)\n",
        "\n",
        "print(\"Streaming Speed → Gold\")\n",
        "print(f\"ID: {query_speed_gold.id}\")\n",
        "print(f\"Nombre: {query_speed_gold.name}\")\n",
        "print(f\"Activo: {query_speed_gold.isActive}\")\n",
        "\n",
        "\n",
        "# pausa para dejar que procese\n",
        "\n",
        "time.sleep(15)\n",
        "\n",
        "print(\" Progreso del streaming\")\n",
        "print(query_speed_gold.lastProgress)\n",
        "\n",
        "print(\"aparando stream...\")\n",
        "\n",
        "if query_speed_gold.isActive:  # esto lo hice por si el stream se para mientras esta procesando algo y tira error java.lang.InterruptedException que no para la ejecucion del colab.\n",
        "    query_speed_gold.stop()    # ese error solo pasa aca porque el stream se para un momento arbitrario, en un deploy real no se haria\n",
        "    try:\n",
        "        query_speed_gold.awaitTermination(timeout=2)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "audit_speed_layer_results(spark)\n",
        "print(\"Streaming Speed → Gold parado.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S94XmoANYnJC",
        "outputId": "3e702fa4-b5d5-470c-b458-ba7c840fa465"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Comenzando Speed Layer...\n",
            "[INFO] Dimensión Organizaciones cacheada: 80 registros.\n",
            "[INFO] Streaming ejecutandose -> /content/datalake/gold/org_daily_usage_by_service_speed\n",
            "Streaming Speed → Gold\n",
            "ID: a68e2513-f8b4-4e69-83f3-c533a23abb0b\n",
            "Nombre: None\n",
            "Activo: True\n",
            "[STREAM 0] Reporte \n",
            " Input: 360 eventos\n",
            "Validos: 359\n",
            "Dropped (Cost < -0.01): 1 (0.3%)\n",
            "todo pasado a Gold de Speed layer\n",
            "[STREAM 1] Reporte \n",
            " Input: 360 eventos\n",
            "Validos: 359\n",
            "Dropped (Cost < -0.01): 1 (0.3%)\n",
            "todo pasado a Gold de Speed layer\n",
            "[STREAM 2] Reporte \n",
            " Input: 10 eventos\n",
            "Validos: 10\n",
            "todo pasado a Gold de Speed layer\n",
            "[STREAM 3] Reporte \n",
            " Input: 9 eventos\n",
            "Validos: 9\n",
            "todo pasado a Gold de Speed layer\n",
            " Progreso del streaming\n",
            "{'id': 'a68e2513-f8b4-4e69-83f3-c533a23abb0b', 'runId': '2ed8bc51-e5b2-43df-ac04-0deed58f4f07', 'name': None, 'timestamp': '2025-11-25T22:27:15.000Z', 'batchId': 3, 'numInputRows': 360, 'inputRowsPerSecond': 72.0, 'processedRowsPerSecond': 192.41047568145376, 'durationMs': {'addBatch': 1683, 'commitOffsets': 53, 'getBatch': 10, 'latestOffset': 48, 'queryPlanning': 15, 'triggerExecution': 1871, 'walCommit': 55}, 'eventTime': {'avg': '2025-08-02T03:04:24.833Z', 'max': '2025-08-31T23:51:00.000Z', 'min': '2025-07-03T00:48:00.000Z', 'watermark': '2025-08-30T23:51:00.000Z'}, 'stateOperators': [{'operatorName': 'dedupe', 'numRowsTotal': 2412, 'numRowsUpdated': 32, 'allUpdatesTimeMs': 230, 'numRowsRemoved': 0, 'allRemovalsTimeMs': 1, 'commitTimeMs': 846, 'memoryUsedBytes': 509040, 'numRowsDroppedByWatermark': 1135, 'numShufflePartitions': 13, 'numStateStoreInstances': 13, 'customMetrics': {'loadedMapCacheHitCount': 74, 'loadedMapCacheMissCount': 40, 'numDroppedDuplicateRows': 0, 'stateOnCurrentVersionSizeBytes': 470560}}], 'sources': [{'description': 'FileStreamSource[file:/content/datalake/landing/usage_events_stream]', 'startOffset': {'logOffset': 2}, 'endOffset': {'logOffset': 3}, 'latestOffset': None, 'numInputRows': 360, 'inputRowsPerSecond': 72.0, 'processedRowsPerSecond': 192.41047568145376}], 'sink': {'description': 'ForeachBatchSink', 'numOutputRows': -1}}\n",
            "aparando stream...\n",
            "\n",
            " cheque de Speed Layer ---\n",
            "Ruta: /content/datalake/gold/org_daily_usage_by_service_speed, Total acumulado en disco: 1446\n",
            "funcionando todo OK (Datos persistidos correctamente)\n",
            "+------------+---------------+------------+--------------+--------------+---------------+----------+\n",
            "|org_id      |org_name       |service_name|daily_cost_usd|daily_requests|daily_carbon_kg|event_date|\n",
            "+------------+---------------+------------+--------------+--------------+---------------+----------+\n",
            "|org_pnsm43d8|Delta Labs 70  |compute     |11.6645       |130.0000      |0.026          |2025-08-13|\n",
            "|org_dhylurtp|Nimbus Cloud 76|compute     |11.9243       |133.0000      |0.0266         |2025-08-13|\n",
            "|org_5iqvnb4g|Gamma Labs 73  |networking  |0.0092        |0.0000        |2.05E-4        |2025-08-13|\n",
            "+------------+---------------+------------+--------------+--------------+---------------+----------+\n",
            "only showing top 3 rows\n",
            "\n",
            "Streaming Speed → Gold parado.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#por si no estan insalados, voy a lo seguro\n",
        "!pip install cassandra-driver\n",
        "!pip install astrapy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7lsUEkUeSElX",
        "outputId": "34b8ff39-4a6b-49d2-e951-a3fd3f99e2ad"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: cassandra-driver in /usr/local/lib/python3.12/dist-packages (3.29.3)\n",
            "Requirement already satisfied: geomet>=1.1 in /usr/local/lib/python3.12/dist-packages (from cassandra-driver) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from geomet>=1.1->cassandra-driver) (8.3.1)\n",
            "Requirement already satisfied: astrapy in /usr/local/lib/python3.12/dist-packages (2.1.0)\n",
            "Requirement already satisfied: deprecation~=2.1.0 in /usr/local/lib/python3.12/dist-packages (from astrapy) (2.1.0)\n",
            "Requirement already satisfied: h11>=0.16.0 in /usr/local/lib/python3.12/dist-packages (from astrapy) (0.16.0)\n",
            "Requirement already satisfied: httpx<1,>=0.25.2 in /usr/local/lib/python3.12/dist-packages (from httpx[http2]<1,>=0.25.2->astrapy) (0.28.1)\n",
            "Requirement already satisfied: pymongo>=3 in /usr/local/lib/python3.12/dist-packages (from astrapy) (4.15.4)\n",
            "Requirement already satisfied: toml<1.0.0,>=0.10.2 in /usr/local/lib/python3.12/dist-packages (from astrapy) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions>=4.0 in /usr/local/lib/python3.12/dist-packages (from astrapy) (4.15.0)\n",
            "Requirement already satisfied: uuid6>=2024.1.12 in /usr/local/lib/python3.12/dist-packages (from astrapy) (2025.0.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from deprecation~=2.1.0->astrapy) (25.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.25.2->httpx[http2]<1,>=0.25.2->astrapy) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.25.2->httpx[http2]<1,>=0.25.2->astrapy) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.25.2->httpx[http2]<1,>=0.25.2->astrapy) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.25.2->httpx[http2]<1,>=0.25.2->astrapy) (3.11)\n",
            "Requirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.12/dist-packages (from httpx[http2]<1,>=0.25.2->astrapy) (4.3.0)\n",
            "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from pymongo>=3->astrapy) (2.8.0)\n",
            "Requirement already satisfied: hyperframe<7,>=6.1 in /usr/local/lib/python3.12/dist-packages (from h2<5,>=3->httpx[http2]<1,>=0.25.2->astrapy) (6.1.0)\n",
            "Requirement already satisfied: hpack<5,>=4.1 in /usr/local/lib/python3.12/dist-packages (from h2<5,>=3->httpx[http2]<1,>=0.25.2->astrapy) (4.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.25.2->httpx[http2]<1,>=0.25.2->astrapy) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# cassandra_utils.py\n",
        "\n",
        "import os\n",
        "from cassandra.cluster import Cluster\n",
        "from cassandra.auth import PlainTextAuthProvider\n",
        "from dotenv import load_dotenv\n",
        "from pathlib import Path\n",
        "\n",
        "# carga de credenciales\n",
        "load_dotenv(\"/content/creds/cred.env\", override=True)\n",
        "# config\n",
        "SECURE_BUNDLE_PATH = \"/content/creds/secure-connect-proyecto-cloud-analytics.zip\"\n",
        "ASTRA_DB_TOKEN = os.getenv(\"ASTRA_DB_APPLICATION_TOKEN\")\n",
        "KEYSPACE = \"Cloud_analytics_db\"\n",
        "\n",
        "# funcines de implementacion de cassandra: tomar, crear e insertar\n",
        "def get_cassandra_session():\n",
        "\n",
        "    if not Path(SECURE_BUNDLE_PATH).exists():\n",
        "        raise FileNotFoundError(f\"Falta el Secure Connect Bundle en: {SECURE_BUNDLE_PATH}\")\n",
        "\n",
        "    if not ASTRA_DB_TOKEN:\n",
        "        raise RuntimeError(\"No se encontro ASTRA_DB_APPLICATION_TOKEN en cred.env\")\n",
        "\n",
        "    cloud_config = {\n",
        "        'secure_connect_bundle': SECURE_BUNDLE_PATH\n",
        "    }\n",
        "\n",
        "    auth_provider = PlainTextAuthProvider(\"token\", ASTRA_DB_TOKEN)\n",
        "\n",
        "    cluster = Cluster(cloud=cloud_config, auth_provider=auth_provider, protocol_version=4)\n",
        "    session = cluster.connect()\n",
        "    return session\n",
        "\n",
        "def create_schema(session):\n",
        "    print(f\"[CASSANDRA] Creando esquema en keyspace '{KEYSPACE}'...\")\n",
        "\n",
        "    ddl_gold = f\"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS \"{KEYSPACE}\".org_daily_usage_by_service (\n",
        "        org_id text,\n",
        "        usage_date date,\n",
        "        service_name text,\n",
        "        daily_cost_usd double,\n",
        "        daily_requests double,\n",
        "        daily_carbon_kg double,\n",
        "        cost_per_request double,\n",
        "        carbon_per_dollar double,\n",
        "        PRIMARY KEY ((org_id), usage_date, service_name)\n",
        "    ) WITH CLUSTERING ORDER BY (usage_date DESC, service_name ASC);\n",
        "    \"\"\"\n",
        "    session.execute(ddl_gold)\n",
        "    print(\"[CASSANDRA] Tabla 'org_daily_usage_by_service' lista.\")\n",
        "\n",
        "def insert_batch_to_cassandra(rows: list[dict]):\n",
        "    if not rows:\n",
        "        return\n",
        "    session = get_cassandra_session()\n",
        "\n",
        "    query = f\"\"\"\n",
        "    INSERT INTO \"{KEYSPACE}\".org_daily_usage_by_service (\n",
        "        org_id, usage_date, service_name,\n",
        "        daily_cost_usd, daily_requests, daily_carbon_kg,\n",
        "        cost_per_request, carbon_per_dollar\n",
        "    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n",
        "    \"\"\"\n",
        "    prepared = session.prepare(query)\n",
        "\n",
        "    for row in rows:\n",
        "        session.execute(prepared, (\n",
        "            row[\"org_id\"],\n",
        "            row[\"event_date\"],\n",
        "            row[\"service_name\"],\n",
        "            row[\"daily_cost_usd\"],\n",
        "            row[\"daily_requests\"],\n",
        "            row[\"daily_carbon_kg\"],\n",
        "            row[\"cost_per_request\"],\n",
        "            row[\"carbon_per_dollar\"]\n",
        "        ))\n",
        "\n",
        "    session.shutdown()"
      ],
      "metadata": {
        "id": "sRGAIwF_oK81"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "upload_gold_to_cassandra(spark)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wDIovXdfoyH9",
        "outputId": "66754a5c-078b-4e52-ed2e-7fd9396042c9"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[SERVING] Iniciando carga a Cassandra...\n",
            "[CASSANDRA] Creando esquema en keyspace 'Cloud_analytics_db'...\n",
            "[CASSANDRA] Tabla 'org_daily_usage_by_service' lista.\n",
            "[SERVING] Leyendo 816 filas de Gold...\n",
            "[SERVING] Carga completada. 816 registros insertados.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# cassandra_queries.py\n",
        "\n",
        "\n",
        "try:\n",
        "    from cassandra_utils import get_cassandra_session, KEYSPACE\n",
        "except ModuleNotFoundError:\n",
        "    pass\n",
        "\n",
        "def run_business_queries():\n",
        "\n",
        "    print(\"\\n  resultados de negocio \\n\")\n",
        "    session = get_cassandra_session()\n",
        "\n",
        "    # CONSULTA 1: Costos por Org y Servicio (Filtro por una Org específica)\n",
        "\n",
        "    target_org = \"org_c11ertj5\"  # se puede cambiar por cualquier orga. Esto se ppodria hacer en la integracion con BI\n",
        "\n",
        "    print(f\"1. detalle de consumo para {target_org} (ultimos registros)\")\n",
        "    query_1 = f\"\"\"\n",
        "        SELECT usage_date, service_name, daily_cost_usd, daily_requests\n",
        "        FROM \"{KEYSPACE}\".org_daily_usage_by_service\n",
        "        WHERE org_id = '{target_org}'\n",
        "        LIMIT 5\n",
        "    \"\"\"\n",
        "    rows = session.execute(query_1)\n",
        "    for r in rows:\n",
        "        print(f\"{r.usage_date}|{r.service_name:10}|${r.daily_cost_usd:.2f} |{r.daily_requests} reqs\")\n",
        "\n",
        "\n",
        "    # CONSULTA 2: Servicios mas costosos\n",
        "\n",
        "    print(f\"\\n 2. costos (chequeo de datos insertados\")\n",
        "    query_2 = f\"\"\"\n",
        "        SELECT org_id, service_name, daily_cost_usd\n",
        "        FROM \"{KEYSPACE}\".org_daily_usage_by_service\n",
        "        WHERE org_id = '{target_org}'\n",
        "        ORDER BY usage_date DESC\n",
        "        LIMIT 5\n",
        "    \"\"\"\n",
        "    rows = session.execute(query_2)\n",
        "    for r in rows:\n",
        "        print(f\"{r.org_id}|{r.service_name} | costo:${r.daily_cost_usd:.4f}\")\n",
        "\n",
        "    session.shutdown()\n",
        "\n",
        "# Ejecutar reporte final\n",
        "run_business_queries()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abTs-3vgsp-b",
        "outputId": "c3cc7065-3721-43a0-c96d-fd8d99b1af1e"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  resultados de negocio \n",
            "\n",
            "1. detalle de consumo para org_c11ertj5 (ultimos registros)\n",
            "2025-08-31|compute   |$7.66 |120.0 reqs\n",
            "2025-08-31|database  |$5.85 |119.0 reqs\n",
            "2025-08-31|genai     |$31.13 |245.0 reqs\n",
            "2025-08-18|compute   |$0.06 |0.0 reqs\n",
            "2025-08-13|database  |$0.11 |0.0 reqs\n",
            "\n",
            " 2. costos (chequeo de datos insertados\n",
            "org_c11ertj5|compute | costo:$7.6606\n",
            "org_c11ertj5|database | costo:$5.8466\n",
            "org_c11ertj5|genai | costo:$31.1266\n",
            "org_c11ertj5|compute | costo:$0.0571\n",
            "org_c11ertj5|database | costo:$0.1065\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#chequeo fina lde idempotencia\n",
        "print(\"\\n chequeo de idempotencia\")\n",
        "\n",
        "# se cuentan filas actuales en Cassandra\n",
        "session = get_cassandra_session()\n",
        "count_1 = session.execute(f'SELECT count(*) FROM \"{KEYSPACE}\".org_daily_usage_by_service').one()[0]\n",
        "session.shutdown()\n",
        "print(f\"Registros en Cassandra (1era ejecucion): {count_1}\")\n",
        "\n",
        "# se re-ejecuta la carga a cassandra\n",
        "print(\">>> Re-ejecutando carga a Cassandra...\")\n",
        "upload_gold_to_cassandra(spark)\n",
        "\n",
        "# se cuentade nuevo\n",
        "session = get_cassandra_session()\n",
        "count_2 = session.execute(f'SELECT count(*) FROM \"{KEYSPACE}\".org_daily_usage_by_service').one()[0]\n",
        "session.shutdown()\n",
        "print(f\"Registros en Cassandra (2da ejecucion): {count_2}\")\n",
        "\n",
        "if count_1 == count_2:\n",
        "    print(\"idempotencia ok\")\n",
        "else:\n",
        "    print(\"[WARN]se generaron duplicados.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ticQzNDtGKfM",
        "outputId": "cc5334b6-e998-418c-bfe7-a2699edf105e"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " chequeo de idempotencia\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:cassandra.protocol:Server warning: Aggregation query used without partition key\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Registros en Cassandra (1era ejecucion): 876\n",
            ">>> Re-ejecutando carga a Cassandra...\n",
            "\n",
            "[SERVING] Iniciando carga a Cassandra...\n",
            "[CASSANDRA] Creando esquema en keyspace 'Cloud_analytics_db'...\n",
            "[CASSANDRA] Tabla 'org_daily_usage_by_service' lista.\n",
            "[SERVING] Leyendo 816 filas de Gold...\n",
            "[SERVING] Carga completada. 816 registros insertados.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:cassandra.protocol:Server warning: Aggregation query used without partition key\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Registros en Cassandra (2da ejecucion): 876\n",
            "idempotencia ok\n"
          ]
        }
      ]
    }
  ]
}